{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepPoseKit main.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11JwAmjvNnvqFsJOUsTWG7Q_zCRqJssR6","authorship_tag":"ABX9TyOZj4nVDEFCklwxVu/2JDnn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ylmXqkVxrj1j","executionInfo":{"status":"ok","timestamp":1612973045711,"user_tz":300,"elapsed":195,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AMVu46hovJr","executionInfo":{"status":"ok","timestamp":1612973067111,"user_tz":300,"elapsed":21574,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}},"outputId":"0e2d1809-fcc9-479e-f9c8-572cd2549bca"},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","\r\n","%tensorflow_version 1.x\r\n","import tensorflow as tf\r\n","print(\"Using tensorflow v\" + str(tf.__version__))\r\n","\r\n","import sys\r\n","!{sys.executable} -m pip install -U deepposekit pyrealsense2\r\n","\r\n","# HOW TO USE THIS SCRIPT:\r\n","\r\n","# 1. Update the source\r\n","# 2. Make sure annotator is uncommented\r\n","# 3. Maybe change the text scaling\r\n","\r\n","from deepposekit import Annotator\r\n","from deepposekit.io import VideoReader, DataGenerator, initialize_dataset, TrainingGenerator, BaseGenerator\r\n","from deepposekit.io import ImageGenerator, VideoWriter\r\n","from deepposekit.io.utils import merge_new_images\r\n","from deepposekit.annotate import KMeansSampler\r\n","from deepposekit.augment import FlipAxis\r\n","from deepposekit.models import StackedDenseNet, DeepLabCut,StackedHourglass,LEAP\r\n","from deepposekit.models import load_model\r\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\r\n","from deepposekit.callbacks import Logger, ModelCheckpoint\r\n","from scipy.signal import find_peaks\r\n","#import pyrealsense2 as rs\r\n","\r\n","import tqdm\r\n","import cv2\r\n","\r\n","import imgaug.augmenters as iaa\r\n","import imgaug as ia\r\n","\r\n","\r\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n","\r\n","#source = 'chick-toy'\r\n","#HOME = f\"{source}/\"\r\n","\r\n","from os.path import expanduser\r\n","try:\r\n","    import google.colab\r\n","    IN_COLAB = True\r\n","    data_path = '/content/drive/MyDrive/GitHub IU/bam/deepposekit_clone/deepposekit_data_custom/'\r\n","    source = 'human'\r\n","    HOME = data_path + f\"{source}/\"\r\n","    print(HOME, source)\r\n","        \r\n","except:\r\n","    IN_COLAB = False\r\n","    data_path = '/content/drive/MyDrive/GitHub IU/bam/deepposekit_clone/deepposekit_data_custom/'\r\n","    source = 'human'\r\n","    HOME = data_path + f\"{source}/\"\r\n","    print(HOME, source)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Using tensorflow v1.15.2\n","Collecting deepposekit\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/9b/2b62e41f5fb680cf977f8901120a38a4ff5fc9611fbb372fa7211082777c/deepposekit-0.3.9.tar.gz (66kB)\n","\u001b[K     |████████████████████████████████| 71kB 4.5MB/s \n","\u001b[?25hCollecting pyrealsense2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/75/5a27a9902e5138fe9591f1bf663d61bc67ea362a94607b329ad0f61553d9/pyrealsense2-2.41.0.2666-cp36-cp36m-manylinux1_x86_64.whl (70.8MB)\n","\u001b[K     |████████████████████████████████| 70.8MB 40kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from deepposekit) (1.19.5)\n","Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from deepposekit) (3.2.2)\n","Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from deepposekit) (1.1.5)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from deepposekit) (2.10.0)\n","Requirement already satisfied, skipping upgrade: imgaug>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from deepposekit) (0.2.9)\n","Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from deepposekit) (4.1.2.30)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from deepposekit) (3.13)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepposekit) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepposekit) (2.4.7)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepposekit) (1.3.1)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepposekit) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepposekit) (2018.9)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->deepposekit) (1.15.0)\n","Requirement already satisfied, skipping upgrade: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.2.9->deepposekit) (2.4.1)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.2.9->deepposekit) (1.4.1)\n","Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.2.9->deepposekit) (7.0.0)\n","Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.2.9->deepposekit) (0.16.2)\n","Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.2.9->deepposekit) (1.7.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug>=0.2.9->deepposekit) (2.5)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug>=0.2.9->deepposekit) (1.1.1)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug>=0.2.9->deepposekit) (4.4.2)\n","Building wheels for collected packages: deepposekit\n","  Building wheel for deepposekit (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepposekit: filename=deepposekit-0.3.9-cp36-none-any.whl size=105114 sha256=d10fc872fc275802387dafc9fe6a52cc09dd3c806a10873d08f397f58ed90dcb\n","  Stored in directory: /root/.cache/pip/wheels/63/92/3e/96682d235db0100cb3f86a8ddd677756a22fd04f0ee1fe3936\n","Successfully built deepposekit\n","Installing collected packages: deepposekit, pyrealsense2\n","Successfully installed deepposekit-0.3.9 pyrealsense2-2.41.0.2666\n","Num GPUs Available:  1\n","/content/drive/MyDrive/GitHub IU/bam/deepposekit_clone/deepposekit_data_custom/human/ human\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ww-MxZIVqDU-","executionInfo":{"status":"ok","timestamp":1612973067112,"user_tz":300,"elapsed":21566,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}},"outputId":"09190821-6845-48ed-aa19-72e64e0e7e37"},"source":["HOME + f'{source}_annotation_set.h5'"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/GitHub IU/bam/deepposekit_clone/deepposekit_data_custom/human/human_annotation_set.h5'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"bdy91hM7ulwu"},"source":["cap = cv2.VideoCapture(HOME + f'{source}_raw.mp4')\r\n"," \r\n","# fourcc = cv2.VideoWriter_fourcc(*'XVID')\r\n","# out = cv2.VideoWriter(HOME + video_file_path + 'pose_human_test_resized.mp4',fourcc, 30, resize_shape)\r\n","\r\n","out = VideoWriter(HOME + f'{source}.mp4', resize_shape, 'mp4v', 30.0, color=True)\r\n","\r\n","while True:\r\n","    ret, frame = cap.read()\r\n","    if ret == True:\r\n","        b = cv2.resize(frame,resize_shape,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\r\n","        out.write(b)\r\n","    else:\r\n","        break\r\n","    \r\n","cap.release()\r\n","out.close()\r\n","cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgF0kAkHo4iA","executionInfo":{"status":"ok","timestamp":1612973077305,"user_tz":300,"elapsed":195,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}}},"source":["def annotate_dataset(overwrite=False):\r\n","    print(\"[INFO] Preparing Data\")\r\n","    # batch_size must be large else kmeans can't be performed\r\n","    reader = VideoReader(HOME + f'{source}.mp4', batch_size=100, gray=True)\r\n","\r\n","    randomly_sampled_frames = []\r\n","    for idx in tqdm.tqdm(range(len(reader)-1)):\r\n","        batch = reader[idx]\r\n","        random_sample = batch[np.random.choice(batch.shape[0], 10, replace=False)]\r\n","        randomly_sampled_frames.append(random_sample)\r\n","    reader.close()\r\n","\r\n","    randomly_sampled_frames = np.concatenate(randomly_sampled_frames)\r\n","    kmeans = KMeansSampler(n_clusters=10, max_iter=100, n_init=20, batch_size=100, verbose=True)\r\n","    kmeans.fit(randomly_sampled_frames)\r\n","    kmeans_sampled_frames, kmeans_cluster_labels = kmeans.sample_data(randomly_sampled_frames, n_samples_per_label=50)\r\n","\r\n","    try:\r\n","        initialize_dataset(\r\n","            images=kmeans_sampled_frames,\r\n","            datapath=HOME + f'{source}_annotation_set.h5',\r\n","            skeleton=HOME + 'skeleton.csv',\r\n","            overwrite=overwrite\r\n","        )\r\n","    except OSError:\r\n","        print(\"[INFO] Dataset Exists - Passing.\")\r\n","\r\n","    # THIS CANNOT BE DONE FROM WITHIN GOOGLE COLAB. USE PYCHARM or an IDE\r\n","    Annotator(datapath = HOME + f'{source}_annotation_set.h5',\r\n","                   dataset ='images',\r\n","                   skeleton = HOME + 'skeleton.csv',\r\n","                   shuffle_colors = False,\r\n","                   text_scale = 0.2).run()\r\n","\r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBquSTbtrA8v","executionInfo":{"status":"ok","timestamp":1612973077530,"user_tz":300,"elapsed":143,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}}},"source":["\r\n","#annotate_dataset(overwrite=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blGahhS_rfE_","executionInfo":{"status":"ok","timestamp":1612973077879,"user_tz":300,"elapsed":207,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}},"outputId":"fb9c3cb5-342c-43d1-93f1-704b5124af72"},"source":["import os\r\n","os.listdir(HOME)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['skeleton.csv',\n"," 'pose_human_test.mp4',\n"," 'best_model_densenet.h5',\n"," 'predictions.npy',\n"," 'human.mp4',\n"," 'human_annotation_set.h5']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oOfG7rEFrbcy","executionInfo":{"status":"ok","timestamp":1612973078397,"user_tz":300,"elapsed":246,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}},"outputId":"61c5b1e9-abe3-4f38-fd10-b6b347c9c004"},"source":["HOME + f'{source}_annotation_set.h5'"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/GitHub IU/bam/deepposekit_clone/deepposekit_data_custom/human/human_annotation_set.h5'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Qs2jbvr5rAQe","executionInfo":{"status":"ok","timestamp":1612973078986,"user_tz":300,"elapsed":228,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}}},"source":["\r\n","def prepare_model():\r\n","\r\n","    print(\"Loading Data Generator\")\r\n","    data_generator = DataGenerator(HOME + f'{source}_annotation_set.h5', mode=\"annotated\")\r\n","\r\n","    print(\"Creating Data Augmenter\")\r\n","    augmenter = []\r\n","    augmenter.append(FlipAxis(data_generator, axis=0))  # flip image up-down\r\n","    augmenter.append(FlipAxis(data_generator, axis=1))  # flip image left-right\r\n","\r\n","    sometimes = []\r\n","    sometimes.append(iaa.Affine(scale={\"x\": (0.95, 1.05), \"y\": (0.95, 1.05)},\r\n","                                translate_percent={'x': (-0.05, 0.05), 'y': (-0.05, 0.05)},\r\n","                                shear=(-8, 8),\r\n","                                order=ia.ALL,\r\n","                                cval=ia.ALL,\r\n","                                mode=ia.ALL)\r\n","                     )\r\n","    sometimes.append(iaa.Affine(scale=(0.8, 1.2),\r\n","                                mode=ia.ALL,\r\n","                                order=ia.ALL,\r\n","                                cval=ia.ALL)\r\n","                     )\r\n","    augmenter.append(iaa.Sometimes(0.75, sometimes))\r\n","    augmenter.append(iaa.Affine(rotate=(-180, 180),\r\n","                                mode=ia.ALL,\r\n","                                order=ia.ALL,\r\n","                                cval=ia.ALL)\r\n","                     )\r\n","    augmenter = iaa.Sequential(augmenter)\r\n","\r\n","    print(\"Creating Training Generator\")\r\n","    train_generator = TrainingGenerator(generator=data_generator,\r\n","                                        downsample_factor=3,\r\n","                                        augmenter=augmenter,\r\n","                                        sigma=3,\r\n","                                        validation_split=0.3,\r\n","                                        use_graph=True,\r\n","                                        random_seed=1,\r\n","                                        graph_scale=1)\r\n","    print(train_generator.get_config())\r\n","    train_generator.on_epoch_end()\r\n","\r\n","    with tf.device(\"gpu:0\"):\r\n","        print(\"[INFO] Preparing Model\")\r\n","        # SELECT MODEL\r\n","\r\n","        # model = StackedDenseNet(train_generator, n_stacks=5, growth_rate=32, pretrained=True)\r\n","        # model = DeepLabCut(train_generator, backbone=\"resnet50\")\r\n","        # model = DeepLabCut(train_generator, backbone=\"mobilenetv2\", alpha=0.75) # Increase alpha to improve accuracy\r\n","        model = DeepLabCut(train_generator, backbone=\"densenet121\")\r\n","        # model = LEAP(train_generator)\r\n","        # model = StackedHourglass(train_generator)\r\n","\r\n","        model.get_config()\r\n","        reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, verbose=1, patience=20)\r\n","\r\n","        model_checkpoint = ModelCheckpoint(\r\n","            HOME + \"best_model_densenet.h5\",\r\n","            monitor=\"loss\",\r\n","            # monitor=\"loss\" # use if validation_split=0\r\n","            verbose=1,\r\n","            save_best_only=True,\r\n","        )\r\n","        early_stop = EarlyStopping(\r\n","            monitor=\"loss\",\r\n","            # monitor=\"loss\" # use if validation_split=0\r\n","            min_delta=0.001,\r\n","            patience=100,\r\n","            verbose=1\r\n","    )\r\n","\r\n","        print(\"Training model...\")\r\n","        callbacks = [early_stop, reduce_lr, model_checkpoint]\r\n","        model.fit(\r\n","            batch_size=1,\r\n","            validation_batch_size=5,\r\n","            callbacks=callbacks,\r\n","            epochs=1,\r\n","            steps_per_epoch=None,\r\n","        )\r\n","\r\n","        model = load_model(\r\n","            HOME + \"best_model_densenet.h5\",\r\n","            augmenter=augmenter,\r\n","            generator=data_generator,\r\n","        )\r\n","\r\n","        model.fit(\r\n","            batch_size=1,\r\n","            validation_batch_size=5,\r\n","            callbacks=callbacks,\r\n","            epochs=1,\r\n","            steps_per_epoch=None,\r\n","        )"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2acH7dpDo6SN","executionInfo":{"status":"ok","timestamp":1612973264799,"user_tz":300,"elapsed":184366,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}},"outputId":"5ef93873-a1f2-45d0-8eb9-514e6dce8266"},"source":["\r\n","prepare_model()\r\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Loading Data Generator\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/deepposekit/io/DataGenerator.py:81: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  self.annotated = np.all(h5file[\"annotated\"].value, axis=1)\n"],"name":"stderr"},{"output_type":"stream","text":["Creating Data Augmenter\n","Creating Training Generator\n","[INFO] Preparing Model\n","WARNING:tensorflow:Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff39e204c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff39e204c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n","29089792/29084464 [==============================] - 0s 0us/step\n","WARNING:tensorflow:Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff39e204c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff39e204c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method SubpixelMaxima2D.call of <deepposekit.models.layers.subpixel.SubpixelMaxima2D object at 0x7ff34350aa90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method SubpixelMaxima2D.call of <deepposekit.models.layers.subpixel.SubpixelMaxima2D object at 0x7ff34350aa90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deepposekit/models/backend/utils.py:35: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Training model...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/deepposekit/models/engine.py:145: UserWarning: \n","Automatically compiling with default settings: model.compile('adam', 'mse')\n","Call model.compile() manually to use non-default settings.\n","\n","  \"\"\"\\nAutomatically compiling with default settings: model.compile('adam', 'mse')\\n\"\"\"\n","/usr/local/lib/python3.6/dist-packages/deepposekit/models/engine.py:161: UserWarning: No validation set detected, so validation step will not be run and `val_loss` will not be available.\n","  \"No validation set detected, so validation step will not be run and `val_loss` will not be available.\"\n"],"name":"stderr"},{"output_type":"stream","text":["8/9 [=========================>....] - ETA: 5s - loss: 11.3334 \n","Epoch 00001: loss improved from inf to 11.15484, saving model to /content/drive/MyDrive/GitHub IU/bam/deepposekit_clone/deepposekit_data_custom/human/best_model_densenet.h5\n","9/9 [==============================] - 54s 6s/step - loss: 11.1548\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff3231700f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff3231700f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff3231700f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff3231700f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method SubpixelMaxima2D.call of <deepposekit.models.layers.subpixel.SubpixelMaxima2D object at 0x7ff20bd79320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method SubpixelMaxima2D.call of <deepposekit.models.layers.subpixel.SubpixelMaxima2D object at 0x7ff20bd79320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","8/9 [=========================>....] - ETA: 2s - loss: 9.8932\n","Epoch 00001: loss improved from 11.15484 to 9.92928, saving model to /content/drive/MyDrive/GitHub IU/bam/deepposekit_clone/deepposekit_data_custom/human/best_model_densenet.h5\n","9/9 [==============================] - 34s 4s/step - loss: 9.9293\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c81LKvmgrDRe","executionInfo":{"status":"ok","timestamp":1612973443816,"user_tz":300,"elapsed":241,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}}},"source":["\r\n","def create_video():\r\n","    print(\"[INFO] Creating Output Video\")\r\n","\r\n","    with tf.device(\"gpu:0\"):\r\n","        model = load_model(HOME + 'best_model_densenet.h5')\r\n","\r\n","    model_size = tuple(model.input_shape[:2])\r\n","    print(model_size, model_size[::-1])\r\n","    model_size = model_size[::-1] \r\n","\r\n","    print(\"Reading Video...\")\r\n","    reader = VideoReader(HOME + f'{source}.mp4', batch_size=1, gray=True)\r\n","    predictions = model.predict(reader, verbose=1)\r\n","    np.save(HOME + 'predictions.npy', predictions)\r\n","    #############################################\r\n","\r\n","    data_generator = DataGenerator(HOME + f'{source}_annotation_set.h5')\r\n","    predictions = predictions[..., :2]\r\n","    print(predictions.shape)\r\n","\r\n","    cmap = plt.cm.hsv(np.linspace(0, 1, data_generator.keypoints_shape[0]))[:, :3][:, ::-1] * 255\r\n","\r\n","    writer = VideoWriter(HOME + f'{source}_predicted.mp4', model_size, 'mp4v', 30.0, color=True)\r\n","    for frame, keypoints in tqdm.tqdm(zip(reader, predictions)):\r\n","        frame = frame[0]\r\n","        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\r\n","        for i, node in enumerate(data_generator.graph):\r\n","            if node >= 0:\r\n","                pt1 = keypoints[i]\r\n","                pt2 = keypoints[node]\r\n","                cv2.line(frame, (pt1[0], pt1[1]), (pt2[0], pt2[1]), (0, 0, 255), 1, cv2.LINE_AA)\r\n","        for i, keypoint in enumerate(keypoints):\r\n","            keypoint = keypoint.astype(int)\r\n","            cv2.circle(frame, (keypoint[0], keypoint[1]), 1, tuple(cmap[i]), -1, cv2.LINE_AA)\r\n","        writer.write(frame)\r\n","\r\n","    writer.close()\r\n","    reader.close()\r\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_an9IySo7Z6","executionInfo":{"status":"ok","timestamp":1612973527591,"user_tz":300,"elapsed":83101,"user":{"displayName":"Advait Save","photoUrl":"","userId":"08904726702539992993"}},"outputId":"163c826c-9822-405f-8059-ff95b1998be7"},"source":["create_video()\r\n","print(\"[INFO] Process Finished\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[INFO] Creating Output Video\n","WARNING:tensorflow:Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff20555e908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff20555e908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff20555e908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method ImageNetPreprocess.call of <deepposekit.models.layers.deeplabcut.ImageNetPreprocess object at 0x7ff20555e908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method SubpixelMaxima2D.call of <deepposekit.models.layers.subpixel.SubpixelMaxima2D object at 0x7ff1fc4276a0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method SubpixelMaxima2D.call of <deepposekit.models.layers.subpixel.SubpixelMaxima2D object at 0x7ff1fc4276a0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","(320, 640) (640, 320)\n","Reading Video...\n","275/275 [==============================] - 21s 75ms/step\n","(275, 7, 2)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/deepposekit/io/DataGenerator.py:81: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n","  self.annotated = np.all(h5file[\"annotated\"].value, axis=1)\n","275it [00:00, 408.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["[INFO] Process Finished\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"uAA015xfswNS"},"source":[""],"execution_count":null,"outputs":[]}]}